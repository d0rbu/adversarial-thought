# Oracle evaluation configuration
defaults:
- _self_
- oracle: default
- data: dolci_sft

# Experiment metadata
experiment:
  name: oracle_eval
  seed: 42
  output_dir: out/${experiment.name}

# Weights & Biases logging
wandb:
  enabled: true
  project: adversarial-thought
  entity:
  tags:
  - oracle
  - eval

# Hardware settings
hardware:
  device: cuda
  dtype: bfloat16

# Questions configuration
questions:
  # Question split: train, val, or all
  split: train
  # Number of questions to use (null = all)
  n_questions: 5

# LLM Judge configuration
judge:
  model: gpt-5-nano
  max_tokens: 2048

# Context prompts source
context:
  # Source: "dataset" or "custom"
  source: dataset
  # For dataset source - number of samples
  n_samples: 100

# Hydra settings
hydra:
  run:
    dir: hydra
