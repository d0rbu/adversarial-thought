# Oracle evaluation configuration
defaults:
- _self_
- oracle: default
- data: dolci_sft
- model: qwen3_8b

# Experiment metadata
experiment:
  name: oracle_eval
  seed: 0
  output_dir: out/${experiment.name}

# Weights & Biases logging
wandb:
  enabled: true
  project: adversarial-thought
  entity:
  tags:
  - oracle
  - eval

# Hardware settings
hardware:
  device: cuda
  dtype: float16
  load_in_8bit: false

# Questions configuration
questions:
  # Question split: train, val, or all
  split: train
  # Number of questions to use (null = all)
  n_questions:

# LLM Judge configuration
judge:
  model: gpt-5-nano
  max_tokens: 8192
  temperature: 1.0

# Hydra settings
hydra:
  run:
    dir: hydra
