# Gemma 3 1B instruction-tuned model configuration
name: google/gemma-3-1b-it
tokenizer: ${model.name}

# LoRA configuration for efficient finetuning
lora:
  enabled: true
  r: 64
  alpha: 128
  dropout: 0.05
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
  bias: none
  task_type: CAUSAL_LM

# Model loading settings
load_in_8bit: false
load_in_4bit: false
attn_implementation: flash_attention_2
