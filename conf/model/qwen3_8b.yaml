# Qwen 3 8B model configuration
name: Qwen/Qwen3-8B
tokenizer: ${model.name}

# LoRA configuration for efficient finetuning
lora:
  enabled: true
  r: 32
  alpha: 64
  dropout: 0.05
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
  bias: none
  task_type: CAUSAL_LM

# Model loading settings
load_in_8bit: true
load_in_4bit: false
attn_implementation: flash_attention_2
