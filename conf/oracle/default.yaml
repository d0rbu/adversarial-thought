# Default oracle evaluation configuration
# Base model (must match the oracle's training base)
model_name: google/gemma-2-9b-it

# Oracle (verbalizer) LoRA path from HuggingFace
# See: https://huggingface.co/collections/adamkarvonen/activation-oracles
oracle_path: adamkarvonen/checkpoints_latentqa_cls_past_lens_addition_gemma-2-9b-it

# Target adapter path (null = evaluate base model activations)
target_adapter_path:

# Activation collection settings
layer_percent: 50  # Layer to collect from (as % of model depth)
segment_start: -10  # Start position for segment (negative = from end)

# Generation settings
max_new_tokens: 50
temperature: 0.0
do_sample: false

# Batch size
batch_size: 32
