# Oracle evaluation for SFT finetuned model
# Base model (must match the oracle and the finetuned model)
model_name: Qwen/Qwen3-8B

# Oracle LoRA path
oracle_path: adamkarvonen/checkpoints_latentqa_cls_past_lens_addition_Qwen3-8B

# Target adapter - our SFT finetuned model
target_adapter_path: out/sft_baseline

# Which layer to extract activations from (as % of model depth)
layer_percent: 50

# Generation settings
max_new_tokens: 256
temperature: 0.0
do_sample: false

# Batch size
batch_size: 4

# Token indices for activation extraction
token_start_idx: -10
token_end_idx: -2

# Repeat count (applies to segment or full_seq depending on detected input type)
repeats: 10
